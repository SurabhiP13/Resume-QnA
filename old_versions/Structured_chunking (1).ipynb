{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !unzip Resume-SK.zip"
      ],
      "metadata": {
        "id": "LuYOL6F0DeQt"
      },
      "id": "LuYOL6F0DeQt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install docling\n",
        "# !pip install -U sentence-transformers\n",
        "# !pip install rank-bm25\n",
        "# !pip install faiss-cpu\n",
        "# !pip install langchain\n",
        "# !pip install openaiA"
      ],
      "metadata": {
        "id": "HD2BulRfI42S"
      },
      "id": "HD2BulRfI42S",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1d53d14f",
      "metadata": {
        "id": "1d53d14f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import hashlib\n",
        "import statistics\n",
        "\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Constants\n",
        "RESUME_PDF_PATH = \"Submit your resume or CV (File responses)\"\n",
        "RESUME_MARKDOWN_PATH = \"Resume-markdown-docling\"\n",
        "MAX_RESUMES = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF to Markdown Conversion using docling and saving it"
      ],
      "metadata": {
        "id": "Yjkm8UV6OlGj"
      },
      "id": "Yjkm8UV6OlGj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82e2a59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82e2a59",
        "outputId": "497ea776-e8a6-4e1a-fefe-5564c1b1f779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 PDFs to convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 20 files\n",
            "Converted 40 files\n",
            "Conversion complete: 52/52 successful\n"
          ]
        }
      ],
      "source": [
        "# # Basic PDF to Markdown conversion\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Setup paths\n",
        "# pdf_dir = Path(\"Submit your resume or CV (File responses)\")\n",
        "# output_dir = Path(\"Resume-markdown-docling\")\n",
        "# output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# # Get first 200 PDFs\n",
        "# pdf_files = list(pdf_dir.glob(\"*.pdf\"))[:200]\n",
        "# print(f\"Found {len(pdf_files)} PDFs to convert\")\n",
        "\n",
        "# # Convert PDFs\n",
        "# converter = DocumentConverter()\n",
        "# successful = 0\n",
        "\n",
        "# for pdf_file in pdf_files:\n",
        "#     try:\n",
        "#         result = converter.convert(str(pdf_file))\n",
        "#         markdown_content = result.document.export_to_markdown()\n",
        "\n",
        "#         output_file = output_dir / f\"{pdf_file.stem}.md\"\n",
        "#         output_file.write_text(markdown_content, encoding='utf-8')\n",
        "\n",
        "#         successful += 1\n",
        "#         if successful % 20 == 0:\n",
        "#             print(f\"Converted {successful} files\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Failed: {pdf_file.name}\")\n",
        "\n",
        "# print(f\"Conversion complete: {successful}/{len(pdf_files)} successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking the markdowns based on custom rules\n",
        "=> 3+ separated capital letters to be joined => due to conversion between pdf and markdown\n",
        "\n",
        "=> splitting based on common headers in resume such as experience , summary etc using regex + it needs to be after ## => thats how docling does it\n"
      ],
      "metadata": {
        "id": "r2Cm85VyOvZt"
      },
      "id": "r2Cm85VyOvZt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a49d8a",
      "metadata": {
        "id": "94a49d8a",
        "outputId": "80d6ad7a-575b-43e2-b364-4b1e531f649a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 250 chunks from 52 resumes\n",
            "Average chunks per resume: 4.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "markdown_dir=Path(\"Resume-markdown-docling\")\n",
        "def fix_spaced_caps(s: str) -> str:\n",
        "    pattern = re.compile(r'(?<!\\w)(?:[A-Z]\\s+){2,}[A-Z](?!\\w)')  # 3+ capital letters separated by spaces\n",
        "    def _join(m):\n",
        "        return m.group(0).replace(' ', '')\n",
        "    return pattern.sub(_join, s)\n",
        "\n",
        "def container_chunking(content: str, resume_id: str) -> List[Document]:\n",
        "    # Remove image tags\n",
        "    content = re.sub(r'<!-- image -->', '', content)\n",
        "    #Removing spacing S K I L L -> SKILL\n",
        "    # Fix spaced out words like \"e x p e r i e n c e\" or \"E X P E R I E N C E\"\n",
        "    # content = re.sub(r'\\b(\\w)\\s+(\\w)\\s+(\\w)(\\s+\\w)*\\b', lambda m: re.sub(r'\\s+', '', m.group()), content)\n",
        "    content = fix_spaced_caps(content)\n",
        "\n",
        "    containers = [\n",
        "        r'about\\s*me', r'summary', r'profile', r'experience', r'work\\s+experience',\n",
        "        r'education', r'skill[s]?', r'project[s]?', r'achievement[s]?', r'award[s]?',\n",
        "        r'publication[s]?', r'competition[s]?', r'hackathon[s]?'\n",
        "    ]\n",
        "    container_alt = '|'.join(containers)\n",
        "\n",
        "    # Anchor to line start, any H1â€“H6, match only the heading line\n",
        "    pattern = rf'(?=^#{{1,6}}\\s*(?:.*\\b(?:{container_alt})\\b).*$)'\n",
        "    chunks = re.split(pattern, content, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # Filter empty chunks and create documents\n",
        "    docs = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk = chunk.strip()\n",
        "        if chunk and len(chunk) > 50:\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={'resume_id': resume_id, 'chunk_id': i}\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Process all resumes\n",
        "all_chunks = []\n",
        "for md_file in markdown_dir.glob(\"*.md\"):\n",
        "    content = md_file.read_text(encoding='utf-8')\n",
        "    resume_id = md_file.stem\n",
        "    chunks = container_chunking(content, resume_id)\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"Created {len(all_chunks)} chunks from {len(list(markdown_dir.glob('*.md')))} resumes\")\n",
        "print(f\"Average chunks per resume: {len(all_chunks) / len(list(markdown_dir.glob('*.md'))):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757de011",
      "metadata": {
        "id": "757de011"
      },
      "source": [
        "Embedding Models\n",
        "\n",
        "Creating embeddings of each chunk in the document(resume) list and storing them in a pickle file (so don't have to keep running it again)\n",
        "PS only works on colab due to version differences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##CREATING OPEN AI EMBEDDINGS FOR ALL CHUNKS\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = (\n",
        "#     userdata.get(\"OPENAI_API_KEY\") or userdata.get(\"openai_api_key\")\n",
        "# )\n",
        "# client=OpenAI()\n",
        "# def get_openai_embeddings(texts: list[str], model: str = \"text-embedding-3-small\", batch_size: int = 100) -> np.ndarray:\n",
        "#     embeddings: list[np.ndarray] = []\n",
        "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
        "#         batch = texts[i : i + batch_size]\n",
        "#         resp = client.embeddings.create(model=model, input=batch)\n",
        "#         embeddings.extend([np.array(item.embedding, dtype=np.float32) for item in resp.data])\n",
        "#     return np.vstack(embeddings) if embeddings else np.zeros((0, 0), dtype=np.float32)\n",
        "\n",
        "# # prepare chunks (prefer in-memory all_chunks, else load saved chunks)\n",
        "# try:\n",
        "#     chunks = all_chunks  # type: ignore[name-defined]\n",
        "# except NameError:\n",
        "#     if os.path.exists(\"resume_chunks.pkl\"):\n",
        "#         with open(\"resume_chunks.pkl\", \"rb\") as f:\n",
        "#             chunks = pickle.load(f)\n",
        "#     else:\n",
        "#         raise RuntimeError(\"No in-memory chunks and resume_chunks.pkl not found.\")\n",
        "\n",
        "# chunk_texts = [doc.page_content for doc in chunks]\n",
        "# print(f\"Generating OpenAI embeddings for {len(chunk_texts)} chunks...\")\n",
        "\n",
        "# # choose \"text-embedding-3-small\" or \"text-embedding-3-large\"\n",
        "# embeddings = get_openai_embeddings(chunk_texts, model=\"text-embedding-3-small\", batch_size=100)\n",
        "\n",
        "#====================================================================================================================\n",
        "# SAVING THE EMBEDDINGS OF CHUNKS AND STORING IN PICKLE FILE\n",
        "# with open(\"resume_chunks_openai.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(chunks, f)\n",
        "\n",
        "# with open(\"resume_embeddings_openai.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(embeddings, f)\n",
        "\n",
        "# print(f\"Saved {len(chunks)} chunks and embeddings -> resume_chunks_openai.pkl, resume_embeddings_openai.pkl\")\n",
        "# print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "#====================================================================================================================\n",
        "# SACHECKING PURELY BASED ON COSINE SIMILARITY\n",
        "\n",
        "# # retrieval function using cosine similarity\n",
        "# def retrieve_similar_chunks_openai(query: str, top_k: int = 5, model: str = \"text-embedding-3-small\"):\n",
        "#     # ensure embeddings & chunks are loaded in scope\n",
        "#     global embeddings, chunks\n",
        "#     if 'embeddings' not in globals() or embeddings is None or embeddings.size == 0:\n",
        "#         if os.path.exists(\"resume_embeddings_openai.pkl\"):\n",
        "#             with open(\"resume_embeddings_openai.pkl\", \"rb\") as f:\n",
        "#                 embeddings = pickle.load(f)\n",
        "#         else:\n",
        "#             raise RuntimeError(\"Embeddings not loaded. Run embedding generation first.\")\n",
        "#     if 'chunks' not in globals() or chunks is None:\n",
        "#         if os.path.exists(\"resume_chunks_openai.pkl\"):\n",
        "#             with open(\"resume_chunks_openai.pkl\", \"rb\") as f:\n",
        "#                 chunks = pickle.load(f)\n",
        "#         else:\n",
        "#             raise RuntimeError(\"Chunks not loaded. Run embedding generation first.\")\n",
        "\n",
        "\n",
        "#     0# get query embedding\n",
        "#     q_resp = client.embeddings.create(model=model, input=[query])\n",
        "#     q_emb = np.array(q_resp.data[0].embedding, dtype=np.float32).reshape(1, -1)\n",
        "#     sims = cosine_similarity(q_emb, embeddings)[0]\n",
        "#     top_idx = np.argsort(sims)[-top_k:][::-1]\n",
        "#     results = []\n",
        "#     for i in top_idx:\n",
        "#         results.append({\n",
        "#             \"score\": float(sims[i]),\n",
        "#             \"content\": chunks[i].page_content,\n",
        "#             \"metadata\": chunks[i].metadata\n",
        "#         })\n",
        "#     return results\n",
        "\n"
      ],
      "metadata": {
        "id": "JhO5p1xxExQf"
      },
      "id": "JhO5p1xxExQf",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # example usage:\n",
        "# results = retrieve_similar_chunks_openai(\"student who has scored more than 95% in school\", top_k=3)\n",
        "# # for r in results: print(r[\"score\"], r[\"metadata\"][\"resume_id\"], r[\"content\"][:200])"
      ],
      "metadata": {
        "id": "PenSXAMQFsJp"
      },
      "id": "PenSXAMQFsJp",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying Hybrid Retrieval\n",
        "\n",
        "\n",
        "For resume based querying, just semantic search doesn't cut it, a lot of the times, the recruiters are looking for some keywords, with semantic similarity it can't match exact keywords hence we need to combine approaches; keyword matching +semantic similarity"
      ],
      "metadata": {
        "id": "1IGxdgPczByd"
      },
      "id": "1IGxdgPczByd"
    },
    {
      "cell_type": "code",
      "source": [
        " # Load saved chunks and embeddings\n",
        "with open('resume_chunks_openai.pkl', 'rb') as f:\n",
        "    chunks = pickle.load(f)\n",
        "\n",
        "with open('resume_embeddings_openai.pkl', 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "\n",
        "# --- tiny cleaner & tokenizer ---\n",
        "_CLEAN_RX = re.compile(r'[^\\w\\s]')\n",
        "_WS_RX = re.compile(r'\\s+')\n",
        "\n",
        "def clean_and_tokenize(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    text = _CLEAN_RX.sub(' ', text)     # remove punctuation\n",
        "    text = _WS_RX.sub(' ', text).strip()\n",
        "    return [t for t in text.split(' ') if t]\n",
        "\n",
        "class BM25Index:\n",
        "    def __init__(self):\n",
        "        self.bm25 = None\n",
        "        self.docs: List[Document] = []\n",
        "        self.doc_tokens: List[List[str]] = []\n",
        "\n",
        "    def fit(self, chunks: List[Document]):\n",
        "        self.docs = chunks\n",
        "        self.doc_tokens = [clean_and_tokenize(d.page_content) for d in chunks]\n",
        "        self.bm25 = BM25Okapi(self.doc_tokens)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 200) -> List[Dict[str, Any]]:\n",
        "        if self.bm25 is None:\n",
        "            raise RuntimeError(\"Call fit(chunks) before search().\")\n",
        "        q_tokens = clean_and_tokenize(query)\n",
        "        scores = self.bm25.get_scores(q_tokens)\n",
        "        # top-k indices by score\n",
        "        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "        out = []\n",
        "        for rank, i in enumerate(top_idx, 1):\n",
        "            d = self.docs[i]\n",
        "            out.append({\n",
        "                \"rank\": rank,\n",
        "                \"bm25_score\": float(scores[i]),\n",
        "                \"resume_id\": (d.metadata or {}).get(\"resume_id\"),\n",
        "                \"chunk_id\": (d.metadata or {}).get(\"chunk_id\"),\n",
        "                \"preview\": d.page_content[:400]\n",
        "            })\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NRqlni71zP6m"
      },
      "id": "NRqlni71zP6m",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Index()\n",
        "bm25.fit(chunks)  # chunks = your List[Document] from the chunker\n",
        "\n",
        "query = \"Mumbai University\"\n",
        "hits = bm25.search(query, top_k=300)\n",
        "\n",
        "print(f\"BM25 hits: {len(hits)}\")\n",
        "for h in hits[:5]:\n",
        "    print(f\"[{h['rank']:>2}] {h['bm25_score']:.3f}  resume={h['resume_id']}  chunk={h['chunk_id']}\")\n",
        "    print(\"   \", (h[\"preview\"] or \"\").splitlines()[0][:540], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-b1MycG1VyN",
        "outputId": "6e588141-9a70-4b72-89e9-a8d8f3b9772d"
      },
      "id": "u-b1MycG1VyN",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 hits: 249\n",
            "[ 1] 6.464  resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh  chunk=2\n",
            "    ## Education ...\n",
            "[ 2] 5.472  resume=(Shivam Dubey)-Resume - Shivam Shailendra Dubey  chunk=1\n",
            "    ## PROFILE SUMMARY ...\n",
            "[ 3] 5.099  resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar  chunk=1\n",
            "    ## Education ...\n",
            "[ 4] 4.662  resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar  chunk=4\n",
            "    ## Technical Skills ...\n",
            "[ 5] 4.419  resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh  chunk=3\n",
            "    ## Experience ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from langchain.docstore.document import Document\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "api_key = getattr(openai, \"api_key\", None) or os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"OpenAI API key not found. Set openai.api_key or OPENAI_API_KEY env var.\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "class DenseIndexSimple:\n",
        "    def __init__(self):\n",
        "        self.docs: List[Document] = []\n",
        "        self.Xn: np.ndarray | None = None  # L2-normalized embeddings (N, D)\n",
        "        self.dim: int | None = None\n",
        "        self.model_name: str | None = None\n",
        "\n",
        "    def fit(self, chunks: List[Document], embeddings: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        chunks: your List[Document]\n",
        "        embeddings: np.ndarray of shape (N, D) aligned with chunks\n",
        "        model_name: the embedding model used (e.g., 'text-embedding-3-small')\n",
        "        \"\"\"\n",
        "        if embeddings.ndim != 2 or len(chunks) != embeddings.shape[0]:\n",
        "            raise ValueError(\"Embeddings must be 2D and aligned with chunks.\")\n",
        "        X = embeddings.astype(np.float32)\n",
        "        X = np.ascontiguousarray(X)\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
        "        self.Xn = X / norms\n",
        "        self.dim = self.Xn.shape[1]\n",
        "        self.docs = chunks\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def _embed_query(self, query: str) -> np.ndarray:\n",
        "        if not self.model_name:\n",
        "            raise RuntimeError(\"Index not initialized with a model_name. Call fit() first.\")\n",
        "        resp = client.embeddings.create(model=self.model_name, input=[query])\n",
        "        q = np.array(resp.data[0].embedding, dtype=np.float32)\n",
        "        if q.shape[0] != self.dim:\n",
        "            raise ValueError(f\"Query dim {q.shape[0]} != index dim {self.dim}. \"\n",
        "                             f\"Use the same embedding model as indexing.\")\n",
        "        q = q / (np.linalg.norm(q) + 1e-12)\n",
        "        return q  # (D,)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 200) -> list[dict]:\n",
        "        if self.Xn is None:\n",
        "            raise RuntimeError(\"Index empty. Call fit() first.\")\n",
        "\n",
        "        # embed + normalize query\n",
        "        q = self._embed_query(query)                 # (D,)\n",
        "        sims = self.Xn @ q                           # (N,)\n",
        "        n = sims.shape[0]\n",
        "        k = max(1, min(top_k, n))                    # clamp to [1, N]\n",
        "\n",
        "        # For small N, argsort is fine and simpler\n",
        "        top_idx = np.argsort(sims)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for rank, i in enumerate(top_idx, 1):\n",
        "            d = self.docs[i]\n",
        "            results.append({\n",
        "                \"rank\": rank,\n",
        "                \"dense_score\": float(sims[i]),\n",
        "                \"resume_id\": (d.metadata or {}).get(\"resume_id\"),\n",
        "                \"chunk_id\": (d.metadata or {}).get(\"chunk_id\"),\n",
        "                \"preview\": d.page_content[:400],\n",
        "            })\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "JSGtdfXj1W-U"
      },
      "id": "JSGtdfXj1W-U",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You already have:\n",
        "# - chunks: List[Document]\n",
        "# - embeddings: np.ndarray from your earlier OpenAI embedding step\n",
        "# - model_name: e.g., \"text-embedding-3-small\" (must match the one used for embeddings)\n",
        "\n",
        "dense = DenseIndexSimple()\n",
        "dense.fit(chunks, embeddings, model_name=\"text-embedding-3-small\")\n",
        "\n",
        "query = \"mumbai university\"\n",
        "dense_hits = dense.search(query, top_k=300)\n",
        "\n",
        "print(f\"Dense hits: {len(dense_hits)}\")\n",
        "for h in dense_hits[:5]:\n",
        "    print(f\"[{h['rank']:>2}] {h['dense_score']:.4f} resume={h['resume_id']} chunk={h['chunk_id']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKR_LxWJ6h7L",
        "outputId": "3cd1e350-0f28-4d61-ba0a-a2deb21b2f93"
      },
      "id": "UKR_LxWJ6h7L",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense hits: 249\n",
            "[ 1] 0.4334 resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar chunk=1\n",
            "[ 2] 0.3864 resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh chunk=2\n",
            "[ 3] 0.3802 resume=Saaquib Motiwala Resume-6 (1) - saaquib motiwala chunk=3\n",
            "[ 4] 0.3674 resume=Mohit_CV - Mohit Lohani chunk=1\n",
            "[ 5] 0.3660 resume=Resume Mollika - Mollika Garg chunk=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "def _make_key(hit: Dict[str, Any]) -> str:\n",
        "    \"\"\"Stable key to identify a chunk across lists.\"\"\"\n",
        "    rid = (hit.get(\"resume_id\") or \"\").strip()\n",
        "    cid = str(hit.get(\"chunk_id\") or \"\").strip()\n",
        "    if rid or cid:\n",
        "        return f\"{rid}::{cid}\"\n",
        "    # Fallback: hash preview if metadata missing\n",
        "    prev = (hit.get(\"preview\") or \"\")[:256]\n",
        "    return \"hash::\" + hashlib.md5(prev.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def rrf_fuse(\n",
        "    bm25_hits: Optional[List[Dict[str, Any]]],\n",
        "    dense_hits: Optional[List[Dict[str, Any]]],\n",
        "    k: int = 60,\n",
        "    top_k: int = 100,\n",
        "    weights: Dict[str, float] = None,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Reciprocal Rank Fusion (RRF): score = sum_s w_s * 1/(k + rank_s)\n",
        "    - bm25_hits / dense_hits: lists with at least {'rank', 'resume_id', 'chunk_id'} (or 'preview').\n",
        "    - k: stabilization constant (common choices: 60, 100).\n",
        "    - top_k: number of fused results to return.\n",
        "    - weights: optional per-source weights, e.g., {'bm25': 1.0, 'dense': 1.0}\n",
        "    \"\"\"\n",
        "    weights = weights or {\"bm25\": 1.0, \"dense\": 1.0}\n",
        "    pool: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    def add_source(hits: Optional[List[Dict[str, Any]]], label: str):\n",
        "        if not hits:\n",
        "            return\n",
        "        for h in hits:\n",
        "            key = _make_key(h)\n",
        "            rec = pool.setdefault(key, {\n",
        "                \"resume_id\": h.get(\"resume_id\"),\n",
        "                \"chunk_id\": h.get(\"chunk_id\"),\n",
        "                \"preview\": h.get(\"preview\"),\n",
        "                # keep original per-source info if present\n",
        "                \"bm25_rank\": None, \"bm25_score\": None,\n",
        "                \"dense_rank\": None, \"dense_score\": None,\n",
        "                \"rrf_score\": 0.0,\n",
        "            })\n",
        "            r = h.get(\"rank\")\n",
        "            if isinstance(r, int) and r >= 1:\n",
        "                rec[\"rrf_score\"] += weights.get(label, 1.0) * (1.0 / (k + r))\n",
        "            # stash per-source details (first occurrence wins)\n",
        "            rank_key = f\"{label}_rank\"\n",
        "            score_key = f\"{label}_score\"\n",
        "            if rec[rank_key] is None:\n",
        "                rec[rank_key] = r\n",
        "            if rec[score_key] is None:\n",
        "                # hit may have 'bm25_score' or 'dense_score'\n",
        "                val = h.get(score_key) or h.get(\"bm25_score\") or h.get(\"dense_score\")\n",
        "                rec[score_key] = float(val) if val is not None else None\n",
        "\n",
        "    add_source(bm25_hits, \"bm25\")\n",
        "    add_source(dense_hits, \"dense\")\n",
        "\n",
        "    fused = sorted(pool.values(), key=lambda x: x[\"rrf_score\"], reverse=True)[:top_k]\n",
        "    # add final rank\n",
        "    for i, rec in enumerate(fused, 1):\n",
        "        rec[\"rank\"] = i\n",
        "    return fused\n"
      ],
      "metadata": {
        "id": "dsb8AnKfD5LH"
      },
      "id": "dsb8AnKfD5LH",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"A graduate from Mumbai University\"\n",
        "bm25_hits = bm25.search(query, top_k=500)\n",
        "dense_hits = dense.search(query, top_k=300)\n",
        "\n",
        "fused = rrf_fuse(bm25_hits, dense_hits, k=60, top_k=100)\n",
        "for r in fused[:10]:\n",
        "    print(f\"[{r['rank']:>2}] RRF={r['rrf_score']:.5f}  bm25_r={r['bm25_rank']}  dense_r={r['dense_rank']}\")\n",
        "    print(\"   \", (r[\"preview\"] or \"\").splitlines()[0][:120], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxRZEFNMHvMp",
        "outputId": "1549849d-3390-41a4-fa31-5b80f4010281"
      },
      "id": "VxRZEFNMHvMp",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1] RRF=0.03227  bm25_r=1  dense_r=3\n",
            "    ## PROFILE SUMMARY ...\n",
            "[ 2] RRF=0.03200  bm25_r=3  dense_r=2\n",
            "    ## Education ...\n",
            "[ 3] RRF=0.03175  bm25_r=2  dense_r=4\n",
            "    ## Education ...\n",
            "[ 4] RRF=0.03089  bm25_r=9  dense_r=1\n",
            "    ## EDUCATION ...\n",
            "[ 5] RRF=0.02797  bm25_r=11  dense_r=12\n",
            "    ## EDUCATIOn ...\n",
            "[ 6] RRF=0.02722  bm25_r=12  dense_r=15\n",
            "    ## Anshuman Awasthi MTech(Computer Science) Indian Institute of Information Technology Lucknow ...\n",
            "[ 7] RRF=0.02699  bm25_r=4  dense_r=28\n",
            "    ## Technical Skills ...\n",
            "[ 8] RRF=0.02586  bm25_r=28  dense_r=9\n",
            "    ## Education ...\n",
            "[ 9] RRF=0.02579  bm25_r=34  dense_r=6\n",
            "    ## Education: ...\n",
            "[10] RRF=0.02558  bm25_r=27  dense_r=11\n",
            "    ## Education ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in fused[:10]:\n",
        "    print(r.get(\"resume_id\", \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r5PeUQyH0k3",
        "outputId": "d88fcac7-4759-4df0-eb84-cddd8b1301f1"
      },
      "id": "6r5PeUQyH0k3",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Shivam Dubey)-Resume - Shivam Shailendra Dubey\n",
            "VES_Ganesh_Deulkar_Resume - Ganesh Deulkar\n",
            "SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh\n",
            "Saaquib Motiwala Resume-6 (1) - saaquib motiwala\n",
            "Mohit_CV - Mohit Lohani\n",
            "Anshuman_Resume_Final 3 - ANSHUMAN AWASTHI\n",
            "VES_Ganesh_Deulkar_Resume - Ganesh Deulkar\n",
            "Resume_Shikhar_USAR(G.G.S.I.P.U) - Shikhar Kanojia\n",
            "Asmita  Bele resume - Asmita Bele\n",
            "resume (2) - Diksha Uniyal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zat9NMq5Poh_"
      },
      "id": "zat9NMq5Poh_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}