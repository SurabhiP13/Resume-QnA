{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !unzip Resume-SK.zip"
      ],
      "metadata": {
        "id": "LuYOL6F0DeQt"
      },
      "id": "LuYOL6F0DeQt",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docling\n",
        "!pip install -U sentence-transformers\n",
        "!pip install rank-bm25\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "HD2BulRfI42S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae72943-96d1-4d90-d7f6-04026b05f3ba"
      },
      "id": "HD2BulRfI42S",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.55.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.11.9)\n",
            "Collecting docling-core<3.0.0,>=2.48.2 (from docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading docling_core-2.48.4-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.4.0 (from docling)\n",
            "  Downloading docling_parse-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.9.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.11.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.35.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting easyocr<2.0,>=1.7 (from docling)\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2025.8.3)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typer<0.20.0,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from docling) (0.19.2)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (5.4.0)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.2)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.10.1)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-2.22.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.8.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (4.25.1)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.48.2->docling) (4.56.2)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.23.0+cu126)\n",
            "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (4.12.0.88)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
            "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr<2.0,>=1.7->docling) (2.1.2)\n",
            "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.1.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-37.11.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.22.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.9.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.3.8)\n",
            "Downloading docling-2.55.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.48.4-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.9.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-2.22.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.11.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=40ba3ebd305165ed14e6d7af059e96cbb423ab391068cfb7d27856c5b26e1c43\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: python-bidi, pylatexenc, pyclipper, filetype, XlsxWriter, rtree, python-docx, pypdfium2, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, faker, python-pptx, polyfactory, semchunk, docling-core, easyocr, docling-parse, docling-ibm-models, docling\n",
            "Successfully installed XlsxWriter-3.2.9 docling-2.55.1 docling-core-2.48.4 docling-ibm-models-3.9.1 docling-parse-4.5.0 easyocr-1.7.2 faker-37.11.0 filetype-1.2.0 jsonlines-3.1.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.0 mpire-2.10.2 ninja-1.13.0 polyfactory-2.22.2 pyclipper-1.3.0.post6 pylatexenc-2.10 pypdfium2-4.30.0 python-bidi-0.6.6 python-docx-1.2.0 python-pptx-1.0.2 rtree-1.4.1 semchunk-2.2.2\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank-bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1d53d14f",
      "metadata": {
        "id": "1d53d14f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import hashlib\n",
        "import statistics\n",
        "\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Constants\n",
        "RESUME_PDF_PATH = \"Submit your resume or CV (File responses)\"\n",
        "RESUME_MARKDOWN_PATH = \"Resume-markdown-docling\"\n",
        "MAX_RESUMES = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f82e2a59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82e2a59",
        "outputId": "497ea776-e8a6-4e1a-fefe-5564c1b1f779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 PDFs to convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 20 files\n",
            "Converted 40 files\n",
            "Conversion complete: 52/52 successful\n"
          ]
        }
      ],
      "source": [
        "# Basic PDF to Markdown conversion\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup paths\n",
        "pdf_dir = Path(\"Submit your resume or CV (File responses)\")\n",
        "output_dir = Path(\"Resume-markdown-docling\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Get first 200 PDFs\n",
        "pdf_files = list(pdf_dir.glob(\"*.pdf\"))[:200]\n",
        "print(f\"Found {len(pdf_files)} PDFs to convert\")\n",
        "\n",
        "# Convert PDFs\n",
        "converter = DocumentConverter()\n",
        "successful = 0\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    try:\n",
        "        result = converter.convert(str(pdf_file))\n",
        "        markdown_content = result.document.export_to_markdown()\n",
        "\n",
        "        output_file = output_dir / f\"{pdf_file.stem}.md\"\n",
        "        output_file.write_text(markdown_content, encoding='utf-8')\n",
        "\n",
        "        successful += 1\n",
        "        if successful % 20 == 0:\n",
        "            print(f\"Converted {successful} files\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {pdf_file.name}\")\n",
        "\n",
        "print(f\"Conversion complete: {successful}/{len(pdf_files)} successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "94a49d8a",
      "metadata": {
        "id": "94a49d8a",
        "outputId": "80d6ad7a-575b-43e2-b364-4b1e531f649a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 250 chunks from 52 resumes\n",
            "Average chunks per resume: 4.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "markdown_dir=Path(\"Resume-markdown-docling\")\n",
        "def fix_spaced_caps(s: str) -> str:\n",
        "    pattern = re.compile(r'(?<!\\w)(?:[A-Z]\\s+){2,}[A-Z](?!\\w)')  # 3+ capital letters separated by spaces\n",
        "    def _join(m):\n",
        "        return m.group(0).replace(' ', '')\n",
        "    return pattern.sub(_join, s)\n",
        "\n",
        "def container_chunking(content: str, resume_id: str) -> List[Document]:\n",
        "    # Remove image tags\n",
        "    content = re.sub(r'<!-- image -->', '', content)\n",
        "    #Removing spacing S K I L L -> SKILL\n",
        "    # Fix spaced out words like \"e x p e r i e n c e\" or \"E X P E R I E N C E\"\n",
        "    # content = re.sub(r'\\b(\\w)\\s+(\\w)\\s+(\\w)(\\s+\\w)*\\b', lambda m: re.sub(r'\\s+', '', m.group()), content)\n",
        "    content = fix_spaced_caps(content)\n",
        "\n",
        "    containers = [\n",
        "        r'about\\s*me', r'summary', r'profile', r'experience', r'work\\s+experience',\n",
        "        r'education', r'skill[s]?', r'project[s]?', r'achievement[s]?', r'award[s]?',\n",
        "        r'publication[s]?', r'competition[s]?', r'hackathon[s]?'\n",
        "    ]\n",
        "    container_alt = '|'.join(containers)\n",
        "\n",
        "    # Anchor to line start, any H1–H6, match only the heading line\n",
        "    pattern = rf'(?=^#{{1,6}}\\s*(?:.*\\b(?:{container_alt})\\b).*$)'\n",
        "    chunks = re.split(pattern, content, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # Filter empty chunks and create documents\n",
        "    docs = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk = chunk.strip()\n",
        "        if chunk and len(chunk) > 50:\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={'resume_id': resume_id, 'chunk_id': i}\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Process all resumes\n",
        "all_chunks = []\n",
        "for md_file in markdown_dir.glob(\"*.md\"):\n",
        "    content = md_file.read_text(encoding='utf-8')\n",
        "    resume_id = md_file.stem\n",
        "    chunks = container_chunking(content, resume_id)\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"Created {len(all_chunks)} chunks from {len(list(markdown_dir.glob('*.md')))} resumes\")\n",
        "print(f\"Average chunks per resume: {len(all_chunks) / len(list(markdown_dir.glob('*.md'))):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757de011",
      "metadata": {
        "id": "757de011"
      },
      "source": [
        "Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75174523",
      "metadata": {
        "id": "75174523"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Store API key in Colab secrets, then access it\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# use existing openai.api_key if set, else read from env\n",
        "api_key = getattr(openai, \"api_key\", None) or os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"OpenAI API key not found. Set openai.api_key or OPENAI_API_KEY env var.\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def get_openai_embeddings(texts: list[str], model: str = \"text-embedding-3-small\", batch_size: int = 100) -> np.ndarray:\n",
        "    embeddings: list[np.ndarray] = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        resp = client.embeddings.create(model=model, input=batch)\n",
        "        embeddings.extend([np.array(item.embedding, dtype=np.float32) for item in resp.data])\n",
        "    return np.vstack(embeddings) if embeddings else np.zeros((0, 0), dtype=np.float32)\n",
        "\n",
        "# prepare chunks (prefer in-memory all_chunks, else load saved chunks)\n",
        "try:\n",
        "    chunks = all_chunks  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    if os.path.exists(\"resume_chunks.pkl\"):\n",
        "        with open(\"resume_chunks.pkl\", \"rb\") as f:\n",
        "            chunks = pickle.load(f)\n",
        "    else:\n",
        "        raise RuntimeError(\"No in-memory chunks and resume_chunks.pkl not found.\")\n",
        "\n",
        "chunk_texts = [doc.page_content for doc in chunks]\n",
        "print(f\"Generating OpenAI embeddings for {len(chunk_texts)} chunks...\")\n",
        "\n",
        "# choose \"text-embedding-3-small\" or \"text-embedding-3-large\"\n",
        "embeddings = get_openai_embeddings(chunk_texts, model=\"text-embedding-3-small\", batch_size=100)\n",
        "\n",
        "# save chunks and embeddings\n",
        "with open(\"resume_chunks_openai.pkl\", \"wb\") as f:\n",
        "    pickle.dump(chunks, f)\n",
        "\n",
        "with open(\"resume_embeddings_openai.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embeddings, f)\n",
        "\n",
        "print(f\"Saved {len(chunks)} chunks and embeddings -> resume_chunks_openai.pkl, resume_embeddings_openai.pkl\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# retrieval function using cosine similarity\n",
        "def retrieve_similar_chunks_openai(query: str, top_k: int = 5, model: str = \"text-embedding-3-small\"):\n",
        "    # ensure embeddings & chunks are loaded in scope\n",
        "    global embeddings, chunks\n",
        "    if 'embeddings' not in globals() or embeddings is None or embeddings.size == 0:\n",
        "        if os.path.exists(\"resume_embeddings_openai.pkl\"):\n",
        "            with open(\"resume_embeddings_openai.pkl\", \"rb\") as f:\n",
        "                embeddings = pickle.load(f)\n",
        "        else:\n",
        "            raise RuntimeError(\"Embeddings not loaded. Run embedding generation first.\")\n",
        "    if 'chunks' not in globals() or chunks is None:\n",
        "        if os.path.exists(\"resume_chunks_openai.pkl\"):\n",
        "            with open(\"resume_chunks_openai.pkl\", \"rb\") as f:\n",
        "                chunks = pickle.load(f)\n",
        "        else:\n",
        "            raise RuntimeError(\"Chunks not loaded. Run embedding generation first.\")\n",
        "\n",
        "\n",
        "    0# get query embedding\n",
        "    q_resp = client.embeddings.create(model=model, input=[query])\n",
        "    q_emb = np.array(q_resp.data[0].embedding, dtype=np.float32).reshape(1, -1)\n",
        "    sims = cosine_similarity(q_emb, embeddings)[0]\n",
        "    top_idx = np.argsort(sims)[-top_k:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"score\": float(sims[i]),\n",
        "            \"content\": chunks[i].page_content,\n",
        "            \"metadata\": chunks[i].metadata\n",
        "        })\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhO5p1xxExQf",
        "outputId": "d18b1097-8de9-423f-c5f1-9e2f9e711f21"
      },
      "id": "JhO5p1xxExQf",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating OpenAI embeddings for 250 chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 3/3 [00:05<00:00,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 250 chunks and embeddings -> resume_chunks_openai.pkl, resume_embeddings_openai.pkl\n",
            "Embeddings shape: (250, 1536)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage:\n",
        "results = retrieve_similar_chunks_openai(\"student who has scored more than 95% in school\", top_k=3)\n",
        "# for r in results: print(r[\"score\"], r[\"metadata\"][\"resume_id\"], r[\"content\"][:200])"
      ],
      "metadata": {
        "id": "PenSXAMQFsJp"
      },
      "id": "PenSXAMQFsJp",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaVAPGMbGbva",
        "outputId": "1bc5b8a4-c761-4ed6-cb84-3351a3b2454d"
      },
      "id": "qaVAPGMbGbva",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5160472393035889,\n",
              "  'content': '## EDUCATION\\n\\n|   Year | Degree/Exam                 | Institute                                  | CGPA/Marks   |\\n|--------|-----------------------------|--------------------------------------------|--------------|\\n|   2026 | Dual Degree (B.Tech+M.Tech) | IIT Kharagpur                              | 8.80/ 10     |\\n|   2021 | CLASS XII                   | Christ Church Boys Senior Secondary School | 95.60%       |\\n|   2019 | CLASS X                     | Christ Church Boys Senior Secondary School | 96.40%       |',\n",
              "  'metadata': {'resume_id': 'zaid_resume_data - Zaid Ahmed Khan',\n",
              "   'chunk_id': 1}},\n",
              " {'score': 0.47852623462677,\n",
              "  'content': '## EDUCATION\\n\\nD G VAISHNAV COLLEGE [2017-2020]\\n\\nBachelor of Science in Physics with Computer Applications OOPS, Basics of C++, JAVA, Data structure Marks obtained - 71%\\n\\nICF SILVER JUBILEE MATRICULATION HIGHER SECONDARY SCHOOL HSC - STATE BOARD [2016-2017] BIOLOGY / MATHS Marks obtained - 84%\\n\\nICF SILVER JUBILEE MATRICULATION HIGHER SECONDARY SCHOOL SSLC - STATE BOARD [2014-2015] MARKS OBTAINED - 92%',\n",
              "  'metadata': {'resume_id': 'DIVYABHARATHI S 2024 resume pdf - Divyabharathi S',\n",
              "   'chunk_id': 4}},\n",
              " {'score': 0.47680357098579407,\n",
              "  'content': '## EDUCATION\\n\\n- Sreenidhi Institute of Science and Technology\\n\\nBachelor of Technology - Electronics and Computer Engineering ◦ GPA: 7.55\\n\\n- Narayana Junior College\\n- Narayana Concept School\\n\\nIntermediate - MPC\\n\\n- Percentage: 96.3\\n\\nSchool\\n\\n- GPA: 9.3',\n",
              "  'metadata': {'resume_id': 'Sai_Nageswara_Raju_CV - Raju', 'chunk_id': 2}}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying Hybrid Retrieval"
      ],
      "metadata": {
        "id": "1IGxdgPczByd"
      },
      "id": "1IGxdgPczByd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved chunks and embeddings\n",
        "with open('resume_chunks_openai.pkl', 'rb') as f:\n",
        "    chunks = pickle.load(f)\n",
        "\n",
        "with open('resume_embeddings_openai.pkl', 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "\n",
        "# --- tiny cleaner & tokenizer ---\n",
        "_CLEAN_RX = re.compile(r'[^\\w\\s]')\n",
        "_WS_RX = re.compile(r'\\s+')\n",
        "\n",
        "def clean_and_tokenize(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    text = _CLEAN_RX.sub(' ', text)     # remove punctuation\n",
        "    text = _WS_RX.sub(' ', text).strip()\n",
        "    return [t for t in text.split(' ') if t]\n",
        "\n",
        "class BM25Index:\n",
        "    def __init__(self):\n",
        "        self.bm25 = None\n",
        "        self.docs: List[Document] = []\n",
        "        self.doc_tokens: List[List[str]] = []\n",
        "\n",
        "    def fit(self, chunks: List[Document]):\n",
        "        self.docs = chunks\n",
        "        self.doc_tokens = [clean_and_tokenize(d.page_content) for d in chunks]\n",
        "        self.bm25 = BM25Okapi(self.doc_tokens)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 200) -> List[Dict[str, Any]]:\n",
        "        if self.bm25 is None:\n",
        "            raise RuntimeError(\"Call fit(chunks) before search().\")\n",
        "        q_tokens = clean_and_tokenize(query)\n",
        "        scores = self.bm25.get_scores(q_tokens)\n",
        "        # top-k indices by score\n",
        "        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "        out = []\n",
        "        for rank, i in enumerate(top_idx, 1):\n",
        "            d = self.docs[i]\n",
        "            out.append({\n",
        "                \"rank\": rank,\n",
        "                \"bm25_score\": float(scores[i]),\n",
        "                \"resume_id\": (d.metadata or {}).get(\"resume_id\"),\n",
        "                \"chunk_id\": (d.metadata or {}).get(\"chunk_id\"),\n",
        "                \"preview\": d.page_content[:400]\n",
        "            })\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NRqlni71zP6m"
      },
      "id": "NRqlni71zP6m",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Index()\n",
        "bm25.fit(chunks)  # chunks = your List[Document] from the chunker\n",
        "\n",
        "query = \"Mumbai University\"\n",
        "hits = bm25.search(query, top_k=300)\n",
        "\n",
        "print(f\"BM25 hits: {len(hits)}\")\n",
        "for h in hits[:5]:\n",
        "    print(f\"[{h['rank']:>2}] {h['bm25_score']:.3f}  resume={h['resume_id']}  chunk={h['chunk_id']}\")\n",
        "    print(\"   \", (h[\"preview\"] or \"\").splitlines()[0][:540], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-b1MycG1VyN",
        "outputId": "6e07f1ad-2a17-495a-a054-92d4b818fdc1"
      },
      "id": "u-b1MycG1VyN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 hits: 249\n",
            "[ 1] 6.464  resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh  chunk=2\n",
            "    ## Education ...\n",
            "[ 2] 5.472  resume=(Shivam Dubey)-Resume - Shivam Shailendra Dubey  chunk=1\n",
            "    ## PROFILE SUMMARY ...\n",
            "[ 3] 5.099  resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar  chunk=1\n",
            "    ## Education ...\n",
            "[ 4] 4.662  resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar  chunk=4\n",
            "    ## Technical Skills ...\n",
            "[ 5] 4.419  resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh  chunk=3\n",
            "    ## Experience ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from langchain.docstore.document import Document\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "api_key = getattr(openai, \"api_key\", None) or os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"OpenAI API key not found. Set openai.api_key or OPENAI_API_KEY env var.\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "class DenseIndexSimple:\n",
        "    def __init__(self):\n",
        "        self.docs: List[Document] = []\n",
        "        self.Xn: np.ndarray | None = None  # L2-normalized embeddings (N, D)\n",
        "        self.dim: int | None = None\n",
        "        self.model_name: str | None = None\n",
        "\n",
        "    def fit(self, chunks: List[Document], embeddings: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        chunks: your List[Document]\n",
        "        embeddings: np.ndarray of shape (N, D) aligned with chunks\n",
        "        model_name: the embedding model used (e.g., 'text-embedding-3-small')\n",
        "        \"\"\"\n",
        "        if embeddings.ndim != 2 or len(chunks) != embeddings.shape[0]:\n",
        "            raise ValueError(\"Embeddings must be 2D and aligned with chunks.\")\n",
        "        X = embeddings.astype(np.float32)\n",
        "        X = np.ascontiguousarray(X)\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
        "        self.Xn = X / norms\n",
        "        self.dim = self.Xn.shape[1]\n",
        "        self.docs = chunks\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def _embed_query(self, query: str) -> np.ndarray:\n",
        "        if not self.model_name:\n",
        "            raise RuntimeError(\"Index not initialized with a model_name. Call fit() first.\")\n",
        "        resp = client.embeddings.create(model=self.model_name, input=[query])\n",
        "        q = np.array(resp.data[0].embedding, dtype=np.float32)\n",
        "        if q.shape[0] != self.dim:\n",
        "            raise ValueError(f\"Query dim {q.shape[0]} != index dim {self.dim}. \"\n",
        "                             f\"Use the same embedding model as indexing.\")\n",
        "        q = q / (np.linalg.norm(q) + 1e-12)\n",
        "        return q  # (D,)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 200) -> list[dict]:\n",
        "        if self.Xn is None:\n",
        "            raise RuntimeError(\"Index empty. Call fit() first.\")\n",
        "\n",
        "        # embed + normalize query\n",
        "        q = self._embed_query(query)                 # (D,)\n",
        "        sims = self.Xn @ q                           # (N,)\n",
        "        n = sims.shape[0]\n",
        "        k = max(1, min(top_k, n))                    # clamp to [1, N]\n",
        "\n",
        "        # For small N, argsort is fine and simpler\n",
        "        top_idx = np.argsort(sims)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for rank, i in enumerate(top_idx, 1):\n",
        "            d = self.docs[i]\n",
        "            results.append({\n",
        "                \"rank\": rank,\n",
        "                \"dense_score\": float(sims[i]),\n",
        "                \"resume_id\": (d.metadata or {}).get(\"resume_id\"),\n",
        "                \"chunk_id\": (d.metadata or {}).get(\"chunk_id\"),\n",
        "                \"preview\": d.page_content[:400],\n",
        "            })\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "JSGtdfXj1W-U"
      },
      "id": "JSGtdfXj1W-U",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You already have:\n",
        "# - chunks: List[Document]\n",
        "# - embeddings: np.ndarray from your earlier OpenAI embedding step\n",
        "# - model_name: e.g., \"text-embedding-3-small\" (must match the one used for embeddings)\n",
        "\n",
        "dense = DenseIndexSimple()\n",
        "dense.fit(chunks, embeddings, model_name=\"text-embedding-3-small\")\n",
        "\n",
        "query = \"mumbai university\"\n",
        "dense_hits = dense.search(query, top_k=300)\n",
        "\n",
        "print(f\"Dense hits: {len(dense_hits)}\")\n",
        "for h in dense_hits[:5]:\n",
        "    print(f\"[{h['rank']:>2}] {h['dense_score']:.4f} resume={h['resume_id']} chunk={h['chunk_id']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKR_LxWJ6h7L",
        "outputId": "a14799bb-870a-4842-9d15-3600669cf8a8"
      },
      "id": "UKR_LxWJ6h7L",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense hits: 249\n",
            "[ 1] 0.4334 resume=VES_Ganesh_Deulkar_Resume - Ganesh Deulkar chunk=1\n",
            "[ 2] 0.3864 resume=SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh chunk=2\n",
            "[ 3] 0.3802 resume=Saaquib Motiwala Resume-6 (1) - saaquib motiwala chunk=3\n",
            "[ 4] 0.3674 resume=Mohit_CV - Mohit Lohani chunk=1\n",
            "[ 5] 0.3660 resume=Resume Mollika - Mollika Garg chunk=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "def _make_key(hit: Dict[str, Any]) -> str:\n",
        "    \"\"\"Stable key to identify a chunk across lists.\"\"\"\n",
        "    rid = (hit.get(\"resume_id\") or \"\").strip()\n",
        "    cid = str(hit.get(\"chunk_id\") or \"\").strip()\n",
        "    if rid or cid:\n",
        "        return f\"{rid}::{cid}\"\n",
        "    # Fallback: hash preview if metadata missing\n",
        "    prev = (hit.get(\"preview\") or \"\")[:256]\n",
        "    return \"hash::\" + hashlib.md5(prev.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def rrf_fuse(\n",
        "    bm25_hits: Optional[List[Dict[str, Any]]],\n",
        "    dense_hits: Optional[List[Dict[str, Any]]],\n",
        "    k: int = 60,\n",
        "    top_k: int = 100,\n",
        "    weights: Dict[str, float] = None,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Reciprocal Rank Fusion (RRF): score = sum_s w_s * 1/(k + rank_s)\n",
        "    - bm25_hits / dense_hits: lists with at least {'rank', 'resume_id', 'chunk_id'} (or 'preview').\n",
        "    - k: stabilization constant (common choices: 60, 100).\n",
        "    - top_k: number of fused results to return.\n",
        "    - weights: optional per-source weights, e.g., {'bm25': 1.0, 'dense': 1.0}\n",
        "    \"\"\"\n",
        "    weights = weights or {\"bm25\": 1.0, \"dense\": 1.0}\n",
        "    pool: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    def add_source(hits: Optional[List[Dict[str, Any]]], label: str):\n",
        "        if not hits:\n",
        "            return\n",
        "        for h in hits:\n",
        "            key = _make_key(h)\n",
        "            rec = pool.setdefault(key, {\n",
        "                \"resume_id\": h.get(\"resume_id\"),\n",
        "                \"chunk_id\": h.get(\"chunk_id\"),\n",
        "                \"preview\": h.get(\"preview\"),\n",
        "                # keep original per-source info if present\n",
        "                \"bm25_rank\": None, \"bm25_score\": None,\n",
        "                \"dense_rank\": None, \"dense_score\": None,\n",
        "                \"rrf_score\": 0.0,\n",
        "            })\n",
        "            r = h.get(\"rank\")\n",
        "            if isinstance(r, int) and r >= 1:\n",
        "                rec[\"rrf_score\"] += weights.get(label, 1.0) * (1.0 / (k + r))\n",
        "            # stash per-source details (first occurrence wins)\n",
        "            rank_key = f\"{label}_rank\"\n",
        "            score_key = f\"{label}_score\"\n",
        "            if rec[rank_key] is None:\n",
        "                rec[rank_key] = r\n",
        "            if rec[score_key] is None:\n",
        "                # hit may have 'bm25_score' or 'dense_score'\n",
        "                val = h.get(score_key) or h.get(\"bm25_score\") or h.get(\"dense_score\")\n",
        "                rec[score_key] = float(val) if val is not None else None\n",
        "\n",
        "    add_source(bm25_hits, \"bm25\")\n",
        "    add_source(dense_hits, \"dense\")\n",
        "\n",
        "    fused = sorted(pool.values(), key=lambda x: x[\"rrf_score\"], reverse=True)[:top_k]\n",
        "    # add final rank\n",
        "    for i, rec in enumerate(fused, 1):\n",
        "        rec[\"rank\"] = i\n",
        "    return fused\n"
      ],
      "metadata": {
        "id": "dsb8AnKfD5LH"
      },
      "id": "dsb8AnKfD5LH",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"A graduate from Mumbai University\"\n",
        "bm25_hits = bm25.search(query, top_k=500)\n",
        "dense_hits = dense.search(query, top_k=300)\n",
        "\n",
        "fused = rrf_fuse(bm25_hits, dense_hits, k=60, top_k=100)\n",
        "for r in fused[:10]:\n",
        "    print(f\"[{r['rank']:>2}] RRF={r['rrf_score']:.5f}  bm25_r={r['bm25_rank']}  dense_r={r['dense_rank']}\")\n",
        "    print(\"   \", (r[\"preview\"] or \"\").splitlines()[0][:120], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxRZEFNMHvMp",
        "outputId": "3174877c-30cc-493e-a19f-8f6eea4058c5"
      },
      "id": "VxRZEFNMHvMp",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1] RRF=0.03227  bm25_r=1  dense_r=3\n",
            "    ## PROFILE SUMMARY ...\n",
            "[ 2] RRF=0.03200  bm25_r=3  dense_r=2\n",
            "    ## Education ...\n",
            "[ 3] RRF=0.03175  bm25_r=2  dense_r=4\n",
            "    ## Education ...\n",
            "[ 4] RRF=0.03089  bm25_r=9  dense_r=1\n",
            "    ## EDUCATION ...\n",
            "[ 5] RRF=0.02797  bm25_r=11  dense_r=12\n",
            "    ## EDUCATIOn ...\n",
            "[ 6] RRF=0.02722  bm25_r=12  dense_r=15\n",
            "    ## Anshuman Awasthi MTech(Computer Science) Indian Institute of Information Technology Lucknow ...\n",
            "[ 7] RRF=0.02699  bm25_r=4  dense_r=28\n",
            "    ## Technical Skills ...\n",
            "[ 8] RRF=0.02586  bm25_r=28  dense_r=9\n",
            "    ## Education ...\n",
            "[ 9] RRF=0.02579  bm25_r=34  dense_r=6\n",
            "    ## Education: ...\n",
            "[10] RRF=0.02558  bm25_r=27  dense_r=11\n",
            "    ## Education ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in fused[:10]:\n",
        "    print(r.get(\"resume_id\", \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r5PeUQyH0k3",
        "outputId": "706700b4-2c63-4c97-ad4c-7eaed884d7b1"
      },
      "id": "6r5PeUQyH0k3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Shivam Dubey)-Resume - Shivam Shailendra Dubey\n",
            "VES_Ganesh_Deulkar_Resume - Ganesh Deulkar\n",
            "SHAHNAWAZ_RESUME (4) - shahnawaz Shaikh\n",
            "Saaquib Motiwala Resume-6 (1) - saaquib motiwala\n",
            "Mohit_CV - Mohit Lohani\n",
            "Anshuman_Resume_Final 3 - ANSHUMAN AWASTHI\n",
            "VES_Ganesh_Deulkar_Resume - Ganesh Deulkar\n",
            "Resume_Shikhar_USAR(G.G.S.I.P.U) - Shikhar Kanojia\n",
            "Asmita  Bele resume - Asmita Bele\n",
            "resume (2) - Diksha Uniyal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zat9NMq5Poh_"
      },
      "id": "zat9NMq5Poh_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}